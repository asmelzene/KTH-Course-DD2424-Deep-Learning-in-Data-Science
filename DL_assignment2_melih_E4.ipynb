{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gradient\n",
    "import dataset\n",
    "import computations\n",
    "import layer\n",
    "#from layer import Linear, Softmax, Gradient\n",
    "import network\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(400)\n",
    "np.seterr(over='raise');\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1 = network.Network()\n",
    "cifar = dataset.CIFAR_IMAGES()\n",
    "#asgn1.test_batch_images(cifar_batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exercise - 4 ###\n",
    "# Read in the data & initialize the parameters of the network\n",
    "filePathLocal_labels = 'Dataset/batches.meta'\n",
    "#filePathLocal_data_TRAIN = 'Dataset/data_batch_1'\n",
    "#filePathLocal_data_VALIDATION = 'Dataset/data_batch_2'\n",
    "filePathLocal_data_ALL = ['Dataset/data_batch_1', 'Dataset/data_batch_2', 'Dataset/data_batch_3', 'Dataset/data_batch_4', 'Dataset/data_batch_5']\n",
    "filePathLocal_data_TEST = 'Dataset/test_batch'\n",
    "\n",
    "filePathList = (filePathLocal_data_ALL, filePathLocal_data_TEST)\n",
    "\n",
    "# Read TRAIN, VALIDATION, TEST data into numpy arrays (numpy.ndarray) from local files\n",
    "network1.ReadData_Exercise4(cifar, filePathList)\n",
    "# X = (d, N), Y = (K, N), y = (N,)   # N=number of total images in X\n",
    "# X = (3072, 10000), Y = (10, 10000), y = (10000,)\n",
    "\n",
    "# Find the MEAN and STD of trainX and broadcast them for matrix calculations\n",
    "# trainX_Broadcast_MeanStd = (mean_train_X_broadcast, std_train_X_broadcast)\n",
    "#trainX_Broadcast_MeanStd = network1.MeanStd_Train_X(network1.train_X)\n",
    "\n",
    "# MEL\n",
    "# Transform the INPUT to have zero mean ** Check that one if we need to transfer all of them separately or\n",
    "# only having the normalization as in here??\n",
    "# Normalize all INPUT data by using MEAN and STD of TRAIN DATA\n",
    "'''\n",
    "train_X_Norm = network1.NormalizeData(network1.train_X, trainX_Broadcast_MeanStd)\n",
    "validation_X_Norm = network1.NormalizeData(network1.validation_X, trainX_Broadcast_MeanStd)\n",
    "test_X_Norm = network1.NormalizeData(network1.test_X, trainX_Broadcast_MeanStd)\n",
    "'''\n",
    "\n",
    "train_X_Norm = network1.NormalizeData_Broadcast(network1.train_X, network1.train_X)\n",
    "validation_X_Norm = network1.NormalizeData_Broadcast(network1.validation_X, network1.train_X)\n",
    "test_X_Norm = network1.NormalizeData_Broadcast(network1.test_X, network1.train_X)\n",
    "\n",
    "# mu = 0; d = network1.train_X.shape[0]; m = 50; K = network1.train_Y.shape[0]\n",
    "# we will use only 20 of 3072 to have a dimension reduction in comparing grad_analytic and grad_Numerical\n",
    "#mu = 0; d = 20; m = 50; K = network1.train_Y.shape[0]\n",
    "mu = 0; d = network1.train_X.shape[0]; m = 50; K = network1.train_Y.shape[0]\n",
    "\n",
    "initial_sizes = (mu, d, m, K)\n",
    "\n",
    "sigma1 = 1 / int(np.sqrt(d))\n",
    "sigma2 = 1 / np.sqrt(m)\n",
    "\n",
    "# Generate W1, W2, b1, b2 matrices with initial values\n",
    "#(W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K, sigma1, sigma2)\n",
    "(W1, W2, b1, b2) = network1.Initialize_W_b(initial_sizes, sigma1, sigma2)\n",
    "# W1 = (50, 3072), W2 = (10, 50), b1 = (50, 1), b2 = (10, 1) # if we use the whole dimensions/features\n",
    "# W1 = (50, 20), W2 = (10, 50), b1 = (50, 1), b2 = (10, 1) # if we use 20 dimensions/features\n",
    "# W1 = (m, d), W2 = (K, m), b1 = (m, 1), b2 = (K, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_Broadcast_MeanStd[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.validation_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.train_X.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape check:\n",
      " train_X_Norm=(3072, 45000)\t validation_X_Norm=(3072, 5000)\t test_X_Norm=(3072, 10000)\n",
      " train_Y=(10, 45000)\t\t validation_Y=(10, 5000)\t\t test_Y=(10, 10000)\n",
      " train_y=(45000,)\t\t validation_y=(5000,)\t\t\t test_y=(10000,)\n",
      " W1=(50, 3072)\t\t\t W2=(10, 50)\t\t\t\t b1=(50, 1)\t\t\t b2=(10, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print(\"Mean-STD:\\n trainX_Broadcast_MeanStd[0]=\\n{}\\n\\ntrainX_Broadcast_MeanStd[1]=\\n{}\".\\\n",
    "      format(trainX_Broadcast_MeanStd[0], trainX_Broadcast_MeanStd[1]))\n",
    "print(\"\\n\\nMean-STD:\\n train_X_Norm.mean(axis=1)={}\\n\\n\".format(train_X_Norm.mean(axis=1)))\n",
    "'''\n",
    "print(\"Shape check:\\n train_X_Norm={}\\t validation_X_Norm={}\\t test_X_Norm={}\".format(train_X_Norm.shape, validation_X_Norm.shape, test_X_Norm.shape))\n",
    "print(\" train_Y={}\\t\\t validation_Y={}\\t\\t test_Y={}\".format(network1.train_Y.shape, network1.validation_Y.shape, network1.test_Y.shape))\n",
    "print(\" train_y={}\\t\\t validation_y={}\\t\\t\\t test_y={}\".format(network1.train_y.shape, network1.validation_y.shape, network1.test_y.shape))\n",
    "print(\" W1={}\\t\\t\\t W2={}\\t\\t\\t\\t b1={}\\t\\t\\t b2={}\".format(W1.shape, W2.shape, b1.shape, b2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(network1.validation_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_batch #S1 #W1 #W2 #H #S #P  #Y_batch.shape\n",
    "#print(\" S1={}\\t\\t\\t H={}\\t\\t\\t\\t S={}\\t\\t\\t P={}\".format(S1.shape, H.shape, S.shape, P.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_s = np.exp(S)\n",
    "#print(exp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Plot_Train_Validation_Cost_Accurracy(self, Cost_Train, Cost_Validation, Acc_Train, Acc_Validation):\n",
    "def Plot_Train_Validation_Cost_Accurracy(Cost_Train, Cost_Validation, Acc_Train, Acc_Validation):\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(Cost_Train, 'g-', label='Train')\n",
    "        plt.plot(Cost_Validation, 'r-', label='Validation')\n",
    "        plt.title('Cost Comparison')\n",
    "        plt.xlabel('Step*10')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.legend()\n",
    "        plt.grid('on')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(Acc_Train, 'g-', label='Train')\n",
    "        plt.plot(Acc_Validation, 'r-', label='Validation')\n",
    "        plt.title('Accuracy Comparison')\n",
    "        plt.xlabel('Step*10')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coarse_Search(l_min, l_max):\n",
    "    l = l_min + (l_max - l_min) * np.random.uniform(0,1)\n",
    "    lambda_coarse = pow(10, l)\n",
    "    return lambda_coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cyclical_Coarse(network1, train_X_Norm, validation_X_Norm, n_cycles):\n",
    "    '''\n",
    "    Up until now, we have trained our networks with Vanilla mini-batch gradient descent. \n",
    "    To help speed up training times and avoid time-consuming searches for good values of eta, we will now\n",
    "    implement mini-batch-GD training where the learning rate at each update step is defined in a cylical way\n",
    "    check equations (14) and (15) in the assignment.\n",
    "    '''\n",
    "    #n_epocs = 200      # number of times we will iterate on the entire data (10K images in our case-1 batch)\n",
    "    n_batch = 100       # the size of the mini-batch. in other words, number of images in 1 mini-batch.\n",
    "    #eta = 0.001         # learning rate (step-size)\n",
    "    lambda_cost = 0.01  # regularization coefficient (punishment)\n",
    "    d = train_X_Norm.shape[0]\n",
    "    m = 50 \n",
    "    K = network1.train_Y.shape[0]  # number of classes\n",
    "    eta_min = 1e-5\n",
    "    #eta_max = 1e-1  \n",
    "    eta_max = 2e-2  # MEL: If I use an eta bigger than 2e-2, I receive an overflow error for SOFTMAX \n",
    "    #n_steps = 500                  # number of steps in half a cycle (from eta_min to eta_max) \n",
    "\n",
    "    # Generate W1, W2, b1, b2 matrices with initial values\n",
    "    # (W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K)\n",
    "    sigma1 = 1 / int(np.sqrt(d))\n",
    "    sigma2 = 1 / np.sqrt(m)\n",
    "    #(W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K, sigma1, sigma2)\n",
    "    initial_sizes = [0, d, m, K]\n",
    "    (W1, W2, b1, b2) = network1.Initialize_W_b(initial_sizes, sigma1, sigma2)\n",
    "\n",
    "    W = [W1, W2]\n",
    "    b = [b1, b2]\n",
    "\n",
    "    linearLayer1 = layer.Linear()\n",
    "    reluLayer = layer.ReLU()             # not an exact layer but operational step..\n",
    "    linearLayer2 = layer.Linear()\n",
    "    softmaxLayer = layer.Softmax()       # not an exact layer but operational step..\n",
    "    ReLUlayer = layer.ReLU()\n",
    "    grad = gradient.Gradient()\n",
    "\n",
    "    layers = [linearLayer1, reluLayer, linearLayer2, softmaxLayer]\n",
    "    # total_batch = how many mini-batches will we need to cover the entire training set?\n",
    "    # we will use 45000 images for training = train_X_Norm.shape[1]\n",
    "    # total_batch = 45000/100 = 450\n",
    "    total_batch = int(np.floor(train_X_Norm.shape[1] / n_batch))\n",
    "    \n",
    "    # A full cycle once go up (from eta_min to eta_max) and once go down (from eta_max to eta_min)\n",
    "    # so we multiply by 2\n",
    "    #n_cycles = 1   # corresponds to \"L array\" in 2.L.ns in the assignment .. \n",
    "    # L=the current cycle number ... n_cycles=the total number of cycles to be applied\n",
    "    # so L is an element of n_cycle >> i.e. L = {0, 1, 2, 3} if n_cycles = 4 \n",
    "    # n_epocs=10 for 1 full cycle having n_steps=500 and n_batch=100\n",
    "    n_steps = 2 * total_batch\n",
    "    # n_steps / total_batch = 2 is picked before this value was used as k in exercise-3\n",
    "    # but in exercise-3, we were given the n_steps, now it is calculated and k is given as 2\n",
    "    # so n_epocs = 2 * 2 * 2 = 8 in our case\n",
    "    n_epocs = int(2 * n_cycles * (n_steps / total_batch))\n",
    "    #n_sanity_batch = 1\n",
    "    # we will only use 100 images for sanity check that means there will be only 1 mini-batch\n",
    "    #n_sanity_batch = 1\n",
    "    #n_test_images = n_batch * n_sanity_batch\n",
    "    \n",
    "    #J_epocs_train = np.zeros(n_epocs)         # cost array     - will keep costs per epoc (iteration)\n",
    "    #Accuracy_epocs_train = np.zeros(n_epocs)  # accuracy array - will keep accuracy per epoc (iteration)\n",
    "    \n",
    "    n_records = int(2 * n_cycles * n_steps / 10)\n",
    "    #J_epocs_train = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    #Accuracy_epocs_train = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    '''\n",
    "    J_epocs_train = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_train = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    LambdaCost_train = np.zeros(n_records)\n",
    "    eta_train = np.zeros(n_records)\n",
    "    \n",
    "    J_epocs_validation = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_validation = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    '''\n",
    "\n",
    "    J_epocs_train = np.zeros(n_epocs) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_train = np.zeros(n_epocs) # we will only record once in 10 iteration\n",
    "    LambdaCost_train = np.zeros(n_epocs)\n",
    "    eta_train = np.zeros(n_epocs)\n",
    "    \n",
    "    J_epocs_validation = np.zeros(n_epocs) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_validation = np.zeros(n_epocs) # we will only record once in 10 iteration\n",
    "\n",
    "    # t = iteration number for eta (don't confuse with the epocs iteration!)\n",
    "    t = 0\n",
    "    # cycleID in use.. if n_cycles=0 then cycle max will be 0. else it will be incremented by 1 at each 2*n_steps\n",
    "    cycle_no = 0\n",
    "    n_records = 0\n",
    "    list_params = []\n",
    "    start_time = datetime.datetime.now()\n",
    "    print('lambda_cost, eta, J_train, J_validation, A_train, A_validation')\n",
    "    for e in range(n_epocs):\n",
    "        lambda_cost = Coarse_Search(-5, -1)\n",
    "        for batch in range(total_batch):\n",
    "            #print('cycle_no: {}, e: {}, batch: {}, t: {}, eta: {}'.format(cycle_no, e, batch, t, eta))\n",
    "            index_list = list(range(batch * n_batch, (batch + 1) * n_batch))\n",
    "            # shuffling is not necessary but good to have\n",
    "            np.random.shuffle(index_list)\n",
    "            #X_batch = train_X_Norm[:, index_list]\n",
    "            X_batch = train_X_Norm[:, index_list]\n",
    "            Y_batch = network1.train_Y[:, index_list]\n",
    "\n",
    "            P, H = network1.EvaluationClassifier(layers, X_batch, W, b)\n",
    "\n",
    "            G2 = -np.subtract(network1.train_Y[:, index_list], P)  # G\n",
    "            N2 = Y_batch.shape[1] #N\n",
    "\n",
    "            # N, G\n",
    "            (grad_W2, grad_b2, G1) = grad.ComputeGradients_Linear_HiddenLayer(N2, G2, H, lambda_cost, W2) \n",
    "            G0 = reluLayer.Backward(G1, H)\n",
    "            N1 = H.shape[1] #N\n",
    "            # N, G\n",
    "            (grad_W1, grad_b1) = grad.ComputeGradients_Linear_FirstLayer(N1, G0, X_batch, lambda_cost, W1)\n",
    "            \n",
    "            if 2*cycle_no*n_steps <= t and t <= (2*cycle_no+1)*n_steps:\n",
    "                eta = eta_min + (t - 2*cycle_no*n_steps)/n_steps*(eta_max-eta_min)\n",
    "            elif (2*cycle_no+1)*n_steps <= t and t <= 2*(cycle_no+1)*n_steps:\n",
    "                eta = eta_max - (t - (2*cycle_no+1)*n_steps)/n_steps*(eta_max-eta_min)\n",
    "            \n",
    "            # W1star, W2star\n",
    "            W1 = W1 - eta * grad_W1\n",
    "            W2 = W2 - eta * grad_W2\n",
    "\n",
    "            # b1star, b2star\n",
    "            bstar_m1 = b1 - eta * grad_b1\n",
    "            b1 = bstar_m1[:, :1]  # there's a broadcast issue needs to be fixed, that's why we pick only 1 column\n",
    "\n",
    "            bstar_m2 = b2 - eta * grad_b2\n",
    "            b2 = bstar_m2[:, :1]  # broadcast issue, this is a quick workaround - use 1 column\n",
    "            \n",
    "            #print('cycle_no: {}, e: {}, batch: {}, t: {}, eta: {}'.format(cycle_no, e, batch, t, eta))\n",
    "            \n",
    "            '''\n",
    "            # 12:43 if I calculate J for each t\n",
    "            if t % 10 == 9:    \n",
    "                W = [W1, W2]\n",
    "                b = [b1, b2]\n",
    "                            \n",
    "                eta_train[n_records] = eta\n",
    "                LambdaCost_train[n_records] = lambda_cost\n",
    "\n",
    "                print('\\nepoch: {}, batch: {}, n_records: {}, t: {}'.format(e, batch, n_records, t))\n",
    "                print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "                J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "                J_epocs_train[n_records] = J_train\n",
    "\n",
    "                P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "                k_train = np.argmax(P, axis=0)\n",
    "\n",
    "                A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "                Accuracy_epocs_train[n_records] = A_train\n",
    "\n",
    "                print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "                J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "                J_epocs_validation[n_records] = J_validation\n",
    "\n",
    "                P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "                k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "                A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "                Accuracy_epocs_validation[n_records] = A_validation\n",
    "                \n",
    "                list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "                print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "                list_params.append(list_temp)                \n",
    "                n_records += 1\n",
    "            '''\n",
    "            t += 1\n",
    "            \n",
    "            if t % (2 * n_steps) == 0:\n",
    "                cycle_no += 1\n",
    "\n",
    "        W = [W1, W2]\n",
    "        b = [b1, b2]\n",
    "\n",
    "        eta_train[e] = eta\n",
    "        LambdaCost_train[e] = lambda_cost\n",
    "\n",
    "        print('\\nepoch: {}, batch: {}, t: {}'.format(e, batch, t))\n",
    "        print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "        J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "        J_epocs_train[e] = J_train\n",
    "\n",
    "        P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "        k_train = np.argmax(P, axis=0)\n",
    "\n",
    "        A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "        Accuracy_epocs_train[e] = A_train\n",
    "\n",
    "        print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "        J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "        J_epocs_validation[e] = J_validation\n",
    "\n",
    "        P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "        k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "        A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "        Accuracy_epocs_validation[e] = A_validation\n",
    "                \n",
    "        list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "        print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "        list_params.append(list_temp)                \n",
    "        #n_records += 1\n",
    "        \n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    print(\"Calculation time of Train_Cyclical: \" + str(end_time - start_time))\n",
    "    print(\"x-axis (steps) must be multiplied by 10 since the values recorded once in every 10 Cycle\")\n",
    "        \n",
    "    #print('J_epocs_train: {}'.format(J_epocs_train))\n",
    "    #print('Accuracy_epocs_train: {}'.format(Accuracy_epocs_train))\n",
    "    \n",
    "    Plot_Train_Validation_Cost_Accurracy(J_epocs_train, J_epocs_validation, Accuracy_epocs_train, Accuracy_epocs_validation)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_cost, eta, J_train, J_validation, A_train, A_validation\n",
      "\n",
      "epoch: 0, batch: 449, t: 450\n",
      "time: 2020-05-06 11:20:46.378931 ... operation: J_train\n",
      "time: 2020-05-06 11:24:11.379949 ... operation: J_validation\n",
      "params: [1.0175839366222833e-05, 0.009982788888888889, 26.907630799519847, 26.51228011947062, 0.2132888888888889, 0.2166] ... time: 2020-05-06 11:24:11.880633\n",
      "\n",
      "epoch: 1, batch: 449, t: 900\n",
      "time: 2020-05-06 11:24:12.617747 ... operation: J_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melih/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/Deep Learning/Assignment-2/_codeMel/network.py:206: RuntimeWarning: invalid value encountered in true_divide\n",
      "  P = exp_s / np.sum(exp_s, axis=0)\n",
      "/Users/melih/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/Deep Learning/Assignment-2/_codeMel/network.py:208: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_loss = -np.log(np.dot(Y.transpose(), P))\n",
      "/Users/melih/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/Deep Learning/Assignment-2/_codeMel/layer.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = exp_s / np.sum(exp_s, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2020-05-06 11:25:16.266745 ... operation: J_validation\n",
      "params: [0.043433826635473606, 0.01997778888888889, nan, nan, 0.06722222222222222, 0.062] ... time: 2020-05-06 11:25:16.934901\n"
     ]
    },
    {
     "ename": "FloatingPointError",
     "evalue": "overflow encountered in exp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4762fc02dae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrain_Cyclical_Coarse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X_Norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_X_Norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-7b42a1bcb5ca>\u001b[0m in \u001b[0;36mTrain_Cyclical_Coarse\u001b[0;34m(network1, train_X_Norm, validation_X_Norm, n_cycles)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluationClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mG2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/Deep Learning/Assignment-2/_codeMel/network.py\u001b[0m in \u001b[0;36mEvaluationClassifier\u001b[0;34m(self, layers, X, W, b)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# P = probabilities of each class to the corresponding images  # P = (K, n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;31m#print('P = {}'.format(P.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/Deep Learning/Assignment-2/_codeMel/layer.py\u001b[0m in \u001b[0;36mForward\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;34m\"\"\" Standard definition of the softmax function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# think of making it as a STATIC method, @staticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mexp_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_s\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFloatingPointError\u001b[0m: overflow encountered in exp"
     ]
    }
   ],
   "source": [
    "Train_Cyclical_Coarse(network1, train_X_Norm, validation_X_Norm, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cyclical_Coarse_lambda(network1, train_X_Norm, validation_X_Norm, n_cycles, lambda_coarse):\n",
    "    '''\n",
    "    Up until now, we have trained our networks with Vanilla mini-batch gradient descent. \n",
    "    To help speed up training times and avoid time-consuming searches for good values of eta, we will now\n",
    "    implement mini-batch-GD training where the learning rate at each update step is defined in a cylical way\n",
    "    check equations (14) and (15) in the assignment.\n",
    "    '''\n",
    "    #n_epocs = 200      # number of times we will iterate on the entire data (10K images in our case-1 batch)\n",
    "    n_batch = 100       # the size of the mini-batch. in other words, number of images in 1 mini-batch.\n",
    "    #eta = 0.001         # learning rate (step-size)\n",
    "    lambda_cost = 0.01  # regularization coefficient (punishment)\n",
    "    d = train_X_Norm.shape[0]\n",
    "    m = 50 \n",
    "    K = network1.train_Y.shape[0]  # number of classes\n",
    "    eta_min = 1e-5\n",
    "    #eta_max = 1e-1  \n",
    "    eta_max = 2e-4  # MEL: If I use an eta bigger than 2e-2, I receive an overflow error for SOFTMAX \n",
    "    #n_steps = 500                  # number of steps in half a cycle (from eta_min to eta_max) \n",
    "\n",
    "    # Generate W1, W2, b1, b2 matrices with initial values\n",
    "    # (W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K)\n",
    "    sigma1 = 1 / int(np.sqrt(d))\n",
    "    sigma2 = 1 / np.sqrt(m)\n",
    "    #(W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K, sigma1, sigma2)\n",
    "    initial_sizes = [0, d, m, K]\n",
    "    (W1, W2, b1, b2) = network1.Initialize_W_b(initial_sizes, sigma1, sigma2)\n",
    "\n",
    "    W = [W1, W2]\n",
    "    b = [b1, b2]\n",
    "\n",
    "    linearLayer1 = layer.Linear()\n",
    "    reluLayer = layer.ReLU()             # not an exact layer but operational step..\n",
    "    linearLayer2 = layer.Linear()\n",
    "    softmaxLayer = layer.Softmax()       # not an exact layer but operational step..\n",
    "    ReLUlayer = layer.ReLU()\n",
    "    grad = gradient.Gradient()\n",
    "\n",
    "    layers = [linearLayer1, reluLayer, linearLayer2, softmaxLayer]\n",
    "    # total_batch = how many mini-batches will we need to cover the entire training set?\n",
    "    # we will use 45000 images for training = train_X_Norm.shape[1]\n",
    "    # total_batch = 45000/100 = 450\n",
    "    total_batch = int(np.floor(train_X_Norm.shape[1] / n_batch))\n",
    "    \n",
    "    # A full cycle once go up (from eta_min to eta_max) and once go down (from eta_max to eta_min)\n",
    "    # so we multiply by 2\n",
    "    #n_cycles = 1   # corresponds to \"L array\" in 2.L.ns in the assignment .. \n",
    "    # L=the current cycle number ... n_cycles=the total number of cycles to be applied\n",
    "    # so L is an element of n_cycle >> i.e. L = {0, 1, 2, 3} if n_cycles = 4 \n",
    "    # n_epocs=10 for 1 full cycle having n_steps=500 and n_batch=100\n",
    "    n_steps = 2 * total_batch\n",
    "    # n_steps / total_batch = 2 is picked before this value was used as k in exercise-3\n",
    "    # but in exercise-3, we were given the n_steps, now it is calculated and k is given as 2\n",
    "    # so n_epocs = 2 * 2 * 2 = 8 in our case\n",
    "    n_epocs = int(2 * n_cycles * (n_steps / total_batch))\n",
    "    #n_sanity_batch = 1\n",
    "    # we will only use 100 images for sanity check that means there will be only 1 mini-batch\n",
    "    #n_sanity_batch = 1\n",
    "    #n_test_images = n_batch * n_sanity_batch\n",
    "    \n",
    "    #J_epocs_train = np.zeros(n_epocs)         # cost array     - will keep costs per epoc (iteration)\n",
    "    #Accuracy_epocs_train = np.zeros(n_epocs)  # accuracy array - will keep accuracy per epoc (iteration)\n",
    "    \n",
    "    n_records = int(2 * n_cycles * n_steps / 10)\n",
    "    #J_epocs_train = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    #Accuracy_epocs_train = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    '''\n",
    "    J_epocs_train = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_train = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    LambdaCost_train = np.zeros(n_records)\n",
    "    eta_train = np.zeros(n_records)\n",
    "    \n",
    "    J_epocs_validation = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_validation = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    '''\n",
    "\n",
    "    J_epocs_train = np.zeros(n_epocs) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_train = np.zeros(n_epocs) # we will only record once in 10 iteration\n",
    "    LambdaCost_train = np.zeros(n_epocs)\n",
    "    eta_train = np.zeros(n_epocs)\n",
    "    \n",
    "    J_epocs_validation = np.zeros(n_epocs) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_validation = np.zeros(n_epocs) # we will only record once in 10 iteration\n",
    "\n",
    "    # t = iteration number for eta (don't confuse with the epocs iteration!)\n",
    "    t = 0\n",
    "    # cycleID in use.. if n_cycles=0 then cycle max will be 0. else it will be incremented by 1 at each 2*n_steps\n",
    "    cycle_no = 0\n",
    "    n_records = 0\n",
    "    list_params = []\n",
    "    start_time = datetime.datetime.now()\n",
    "    print('lambda_cost, eta, J_train, J_validation, A_train, A_validation')\n",
    "    lambda_cost = lambda_coarse\n",
    "    for e in range(n_epocs):\n",
    "        for batch in range(total_batch):\n",
    "            #print('cycle_no: {}, e: {}, batch: {}, t: {}, eta: {}'.format(cycle_no, e, batch, t, eta))\n",
    "            index_list = list(range(batch * n_batch, (batch + 1) * n_batch))\n",
    "            # shuffling is not necessary but good to have\n",
    "            np.random.shuffle(index_list)\n",
    "            #X_batch = train_X_Norm[:, index_list]\n",
    "            X_batch = train_X_Norm[:, index_list]\n",
    "            Y_batch = network1.train_Y[:, index_list]\n",
    "\n",
    "            P, H = network1.EvaluationClassifier(layers, X_batch, W, b)\n",
    "\n",
    "            G2 = -np.subtract(network1.train_Y[:, index_list], P)  # G\n",
    "            N2 = Y_batch.shape[1] #N\n",
    "\n",
    "            # N, G\n",
    "            (grad_W2, grad_b2, G1) = grad.ComputeGradients_Linear_HiddenLayer(N2, G2, H, lambda_cost, W2) \n",
    "            G0 = reluLayer.Backward(G1, H)\n",
    "            N1 = H.shape[1] #N\n",
    "            # N, G\n",
    "            (grad_W1, grad_b1) = grad.ComputeGradients_Linear_FirstLayer(N1, G0, X_batch, lambda_cost, W1)\n",
    "            \n",
    "            if 2*cycle_no*n_steps <= t and t <= (2*cycle_no+1)*n_steps:\n",
    "                eta = eta_min + (t - 2*cycle_no*n_steps)/n_steps*(eta_max-eta_min)\n",
    "            elif (2*cycle_no+1)*n_steps <= t and t <= 2*(cycle_no+1)*n_steps:\n",
    "                eta = eta_max - (t - (2*cycle_no+1)*n_steps)/n_steps*(eta_max-eta_min)\n",
    "            \n",
    "            eta = 1e-6\n",
    "            # W1star, W2star\n",
    "            W1 = W1 - eta * grad_W1\n",
    "            W2 = W2 - eta * grad_W2\n",
    "\n",
    "            # b1star, b2star\n",
    "            bstar_m1 = b1 - eta * grad_b1\n",
    "            b1 = bstar_m1[:, :1]  # there's a broadcast issue needs to be fixed, that's why we pick only 1 column\n",
    "\n",
    "            bstar_m2 = b2 - eta * grad_b2\n",
    "            b2 = bstar_m2[:, :1]  # broadcast issue, this is a quick workaround - use 1 column\n",
    "            \n",
    "            #print('cycle_no: {}, e: {}, batch: {}, t: {}, eta: {}'.format(cycle_no, e, batch, t, eta))\n",
    "            \n",
    "            '''\n",
    "            # 12:43 if I calculate J for each t\n",
    "            if t % 10 == 9:    \n",
    "                W = [W1, W2]\n",
    "                b = [b1, b2]\n",
    "                            \n",
    "                eta_train[n_records] = eta\n",
    "                LambdaCost_train[n_records] = lambda_cost\n",
    "\n",
    "                print('\\nepoch: {}, batch: {}, n_records: {}, t: {}'.format(e, batch, n_records, t))\n",
    "                print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "                J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "                J_epocs_train[n_records] = J_train\n",
    "\n",
    "                P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "                k_train = np.argmax(P, axis=0)\n",
    "\n",
    "                A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "                Accuracy_epocs_train[n_records] = A_train\n",
    "\n",
    "                print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "                J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "                J_epocs_validation[n_records] = J_validation\n",
    "\n",
    "                P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "                k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "                A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "                Accuracy_epocs_validation[n_records] = A_validation\n",
    "                \n",
    "                list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "                print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "                list_params.append(list_temp)                \n",
    "                n_records += 1\n",
    "            '''\n",
    "            t += 1\n",
    "            \n",
    "            if t % (2 * n_steps) == 0:\n",
    "                cycle_no += 1\n",
    "\n",
    "        '''\n",
    "        W = [W1, W2]\n",
    "        b = [b1, b2]\n",
    "\n",
    "        eta_train[e] = eta\n",
    "        LambdaCost_train[e] = lambda_cost\n",
    "\n",
    "        print('\\nepoch: {}, batch: {}, t: {}'.format(e, batch, t))\n",
    "        print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "        J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "        J_epocs_train[e] = J_train\n",
    "\n",
    "        P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "        k_train = np.argmax(P, axis=0)\n",
    "\n",
    "        A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "        Accuracy_epocs_train[e] = A_train\n",
    "\n",
    "        print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "        J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "        J_epocs_validation[e] = J_validation\n",
    "\n",
    "        P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "        k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "        A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "        Accuracy_epocs_validation[e] = A_validation\n",
    "                \n",
    "        list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "        print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "        list_params.append(list_temp)                \n",
    "        #n_records += 1\n",
    "        '''\n",
    "\n",
    "    W = [W1, W2]\n",
    "    b = [b1, b2]\n",
    "\n",
    "    #eta_train[e] = eta\n",
    "    #LambdaCost_train[e] = lambda_cost\n",
    "\n",
    "    print('\\nepoch: {}, batch: {}, t: {}'.format(e, batch, t))\n",
    "    print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "    J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "    #J_epocs_train[e] = J_train\n",
    "\n",
    "    P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "    k_train = np.argmax(P, axis=0)\n",
    "\n",
    "    A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "    #Accuracy_epocs_train[e] = A_train\n",
    "\n",
    "    print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "    J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "    #J_epocs_validation[e] = J_validation\n",
    "\n",
    "    P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "    k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "    A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "    #Accuracy_epocs_validation[e] = A_validation\n",
    "                \n",
    "    list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "    print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "    list_params.append(list_temp)\n",
    "        \n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    print(\"Calculation time of Train_Cyclical: \" + str(end_time - start_time))\n",
    "    print(\"x-axis (steps) must be multiplied by 10 since the values recorded once in every 10 Cycle\")\n",
    "        \n",
    "    #print('J_epocs_train: {}'.format(J_epocs_train))\n",
    "    #print('Accuracy_epocs_train: {}'.format(Accuracy_epocs_train))\n",
    "    \n",
    "    #Plot_Train_Validation_Cost_Accurracy(J_epocs_train, J_epocs_validation, Accuracy_epocs_train, Accuracy_epocs_validation)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_cost, eta, J_train, J_validation, A_train, A_validation\n",
      "\n",
      "epoch: 3, batch: 449, t: 1800\n",
      "time: 2020-05-06 12:07:31.594292 ... operation: J_train\n",
      "time: 2020-05-06 12:11:10.323563 ... operation: J_validation\n",
      "params: [0.01, 1e-06, 3.1050863262720654, 3.1092670839080228, 0.10837777777777778, 0.1084] ... time: 2020-05-06 12:11:10.811966\n",
      "Calculation time of Train_Cyclical: 0:03:41.973654\n",
      "x-axis (steps) must be multiplied by 10 since the values recorded once in every 10 Cycle\n"
     ]
    }
   ],
   "source": [
    "Train_Cyclical_Coarse_lambda(network1, train_X_Norm, validation_X_Norm, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_c = Coarse_Search(-5, -1)\n",
    "print(lambda_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cyclical_Coarse_lambda1(network1, train_X_Norm, validation_X_Norm, n_cycles, lambda_coarse):\n",
    "    '''\n",
    "    Up until now, we have trained our networks with Vanilla mini-batch gradient descent. \n",
    "    To help speed up training times and avoid time-consuming searches for good values of eta, we will now\n",
    "    implement mini-batch-GD training where the learning rate at each update step is defined in a cylical way\n",
    "    check equations (14) and (15) in the assignment.\n",
    "    '''\n",
    "    #n_epocs = 200      # number of times we will iterate on the entire data (10K images in our case-1 batch)\n",
    "    n_batch = 100       # the size of the mini-batch. in other words, number of images in 1 mini-batch.\n",
    "    #eta = 0.001         # learning rate (step-size)\n",
    "    lambda_cost = 0.01  # regularization coefficient (punishment)\n",
    "    d = train_X_Norm.shape[0]\n",
    "    m = 50 \n",
    "    K = network1.train_Y.shape[0]  # number of classes\n",
    "    eta_min = 1e-5\n",
    "    #eta_max = 1e-1  \n",
    "    eta_max = 2e-3  # MEL: If I use an eta bigger than 2e-2, I receive an overflow error for SOFTMAX \n",
    "    #n_steps = 500                  # number of steps in half a cycle (from eta_min to eta_max) \n",
    "\n",
    "    # Generate W1, W2, b1, b2 matrices with initial values\n",
    "    # (W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K)\n",
    "    sigma1 = 1 / int(np.sqrt(d))\n",
    "    sigma2 = 1 / np.sqrt(m)\n",
    "    #(W1, W2, b1, b2) = network1.Initialize_W_b(d, m, K, sigma1, sigma2)\n",
    "    initial_sizes = [0, d, m, K]\n",
    "    (W1, W2, b1, b2) = network1.Initialize_W_b(initial_sizes, sigma1, sigma2)\n",
    "\n",
    "    W = [W1, W2]\n",
    "    b = [b1, b2]\n",
    "\n",
    "    linearLayer1 = layer.Linear()\n",
    "    reluLayer = layer.ReLU()             # not an exact layer but operational step..\n",
    "    linearLayer2 = layer.Linear()\n",
    "    softmaxLayer = layer.Softmax()       # not an exact layer but operational step..\n",
    "    ReLUlayer = layer.ReLU()\n",
    "    grad = gradient.Gradient()\n",
    "\n",
    "    layers = [linearLayer1, reluLayer, linearLayer2, softmaxLayer]\n",
    "    # total_batch = how many mini-batches will we need to cover the entire training set?\n",
    "    # we will use 45000 images for training = train_X_Norm.shape[1]\n",
    "    # total_batch = 45000/100 = 450\n",
    "    total_batch = int(np.floor(train_X_Norm.shape[1] / n_batch))\n",
    "    \n",
    "    # A full cycle once go up (from eta_min to eta_max) and once go down (from eta_max to eta_min)\n",
    "    # so we multiply by 2\n",
    "    #n_cycles = 1   # corresponds to \"L array\" in 2.L.ns in the assignment .. \n",
    "    # L=the current cycle number ... n_cycles=the total number of cycles to be applied\n",
    "    # so L is an element of n_cycle >> i.e. L = {0, 1, 2, 3} if n_cycles = 4 \n",
    "    # n_epocs=10 for 1 full cycle having n_steps=500 and n_batch=100\n",
    "    n_steps = 2 * total_batch\n",
    "    # n_steps / total_batch = 2 is picked before this value was used as k in exercise-3\n",
    "    # but in exercise-3, we were given the n_steps, now it is calculated and k is given as 2\n",
    "    # so n_epocs = 2 * 2 * 2 = 8 in our case\n",
    "    n_epocs = int(2 * n_cycles * (n_steps / total_batch))\n",
    "    #n_sanity_batch = 1\n",
    "    # we will only use 100 images for sanity check that means there will be only 1 mini-batch\n",
    "    #n_sanity_batch = 1\n",
    "    #n_test_images = n_batch * n_sanity_batch\n",
    "    \n",
    "    #J_epocs_train = np.zeros(n_epocs)         # cost array     - will keep costs per epoc (iteration)\n",
    "    #Accuracy_epocs_train = np.zeros(n_epocs)  # accuracy array - will keep accuracy per epoc (iteration)\n",
    "    \n",
    "    n_records = int(2 * n_cycles * n_steps / 10)\n",
    "    #J_epocs_train = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    #Accuracy_epocs_train = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    '''\n",
    "    J_epocs_train = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_train = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    LambdaCost_train = np.zeros(n_records)\n",
    "    eta_train = np.zeros(n_records)\n",
    "    \n",
    "    J_epocs_validation = np.zeros(n_records) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_validation = np.zeros(n_records) # we will only record once in 10 iteration\n",
    "    '''\n",
    "\n",
    "    J_epocs_train = np.zeros(n_epocs) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_train = np.zeros(n_epocs) # we will only record once in 10 iteration\n",
    "    LambdaCost_train = np.zeros(n_epocs)\n",
    "    eta_train = np.zeros(n_epocs)\n",
    "    \n",
    "    J_epocs_validation = np.zeros(n_epocs) # cost array     - will keep costs per epoc (iteration)\n",
    "    Accuracy_epocs_validation = np.zeros(n_epocs) # we will only record once in 10 iteration\n",
    "\n",
    "    # t = iteration number for eta (don't confuse with the epocs iteration!)\n",
    "    t = 0\n",
    "    # cycleID in use.. if n_cycles=0 then cycle max will be 0. else it will be incremented by 1 at each 2*n_steps\n",
    "    cycle_no = 0\n",
    "    n_records = 0\n",
    "    list_params = []\n",
    "    start_time = datetime.datetime.now()\n",
    "    lambda_cost = lambda_coarse\n",
    "    print('lambda_cost, eta, J_train, J_validation, A_train, A_validation')\n",
    "    for e in range(n_epocs):\n",
    "        #lambda_cost = Coarse_Search(-5, -1)\n",
    "        for batch in range(total_batch):\n",
    "            #print('cycle_no: {}, e: {}, batch: {}, t: {}, eta: {}'.format(cycle_no, e, batch, t, eta))\n",
    "            index_list = list(range(batch * n_batch, (batch + 1) * n_batch))\n",
    "            # shuffling is not necessary but good to have\n",
    "            np.random.shuffle(index_list)\n",
    "            #X_batch = train_X_Norm[:, index_list]\n",
    "            X_batch = train_X_Norm[:, index_list]\n",
    "            Y_batch = network1.train_Y[:, index_list]\n",
    "\n",
    "            P, H = network1.EvaluationClassifier(layers, X_batch, W, b)\n",
    "\n",
    "            G2 = -np.subtract(network1.train_Y[:, index_list], P)  # G\n",
    "            N2 = Y_batch.shape[1] #N\n",
    "\n",
    "            # N, G\n",
    "            (grad_W2, grad_b2, G1) = grad.ComputeGradients_Linear_HiddenLayer(N2, G2, H, lambda_cost, W2) \n",
    "            G0 = reluLayer.Backward(G1, H)\n",
    "            N1 = H.shape[1] #N\n",
    "            # N, G\n",
    "            (grad_W1, grad_b1) = grad.ComputeGradients_Linear_FirstLayer(N1, G0, X_batch, lambda_cost, W1)\n",
    "            \n",
    "            if 2*cycle_no*n_steps <= t and t <= (2*cycle_no+1)*n_steps:\n",
    "                eta = eta_min + (t - 2*cycle_no*n_steps)/n_steps*(eta_max-eta_min)\n",
    "            elif (2*cycle_no+1)*n_steps <= t and t <= 2*(cycle_no+1)*n_steps:\n",
    "                eta = eta_max - (t - (2*cycle_no+1)*n_steps)/n_steps*(eta_max-eta_min)\n",
    "            \n",
    "            # W1star, W2star\n",
    "            W1 = W1 - eta * grad_W1\n",
    "            W2 = W2 - eta * grad_W2\n",
    "\n",
    "            # b1star, b2star\n",
    "            bstar_m1 = b1 - eta * grad_b1\n",
    "            b1 = bstar_m1[:, :1]  # there's a broadcast issue needs to be fixed, that's why we pick only 1 column\n",
    "\n",
    "            bstar_m2 = b2 - eta * grad_b2\n",
    "            b2 = bstar_m2[:, :1]  # broadcast issue, this is a quick workaround - use 1 column\n",
    "            \n",
    "            #print('cycle_no: {}, e: {}, batch: {}, t: {}, eta: {}'.format(cycle_no, e, batch, t, eta))\n",
    "            \n",
    "            '''\n",
    "            # 12:43 if I calculate J for each t\n",
    "            if t % 10 == 9:    \n",
    "                W = [W1, W2]\n",
    "                b = [b1, b2]\n",
    "                            \n",
    "                eta_train[n_records] = eta\n",
    "                LambdaCost_train[n_records] = lambda_cost\n",
    "\n",
    "                print('\\nepoch: {}, batch: {}, n_records: {}, t: {}'.format(e, batch, n_records, t))\n",
    "                print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "                J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "                J_epocs_train[n_records] = J_train\n",
    "\n",
    "                P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "                k_train = np.argmax(P, axis=0)\n",
    "\n",
    "                A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "                Accuracy_epocs_train[n_records] = A_train\n",
    "\n",
    "                print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "                J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "                J_epocs_validation[n_records] = J_validation\n",
    "\n",
    "                P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "                k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "                A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "                Accuracy_epocs_validation[n_records] = A_validation\n",
    "                \n",
    "                list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "                print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "                list_params.append(list_temp)                \n",
    "                n_records += 1\n",
    "            '''\n",
    "            t += 1\n",
    "            \n",
    "            if t % (2 * n_steps) == 0:\n",
    "                cycle_no += 1\n",
    "\n",
    "        W = [W1, W2]\n",
    "        b = [b1, b2]\n",
    "\n",
    "        eta_train[e] = eta\n",
    "        LambdaCost_train[e] = lambda_cost\n",
    "\n",
    "        print('\\nepoch: {}, batch: {}, t: {}'.format(e, batch, t))\n",
    "        print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_train'))\n",
    "        J_train = network1.Cost(train_X_Norm, network1.train_Y, W, b, lambda_cost)\n",
    "\n",
    "        J_epocs_train[e] = J_train\n",
    "\n",
    "        P, H = network1.EvaluationClassifier(layers, train_X_Norm, W, b)\n",
    "        k_train = np.argmax(P, axis=0)\n",
    "\n",
    "        A_train = network1.ComputeAccuracy(k_train, network1.train_y)\n",
    "        Accuracy_epocs_train[e] = A_train\n",
    "\n",
    "        print('time: {} ... operation: {}'.format(datetime.datetime.now(), 'J_validation'))\n",
    "        J_validation = network1.Cost(validation_X_Norm , network1.validation_Y, W, b, lambda_cost)\n",
    "\n",
    "        J_epocs_validation[e] = J_validation\n",
    "\n",
    "        P_val, H_val = network1.EvaluationClassifier(layers, validation_X_Norm, W, b)\n",
    "        k_validation = np.argmax(P_val, axis=0)\n",
    "\n",
    "        A_validation = network1.ComputeAccuracy(k_validation, network1.validation_y)\n",
    "        Accuracy_epocs_validation[e] = A_validation\n",
    "                \n",
    "        list_temp = [lambda_cost, eta, J_train, J_validation, A_train, A_validation]\n",
    "        print('params: {} ... time: {}'.format(list_temp, datetime.datetime.now()))\n",
    "        list_params.append(list_temp)                \n",
    "        #n_records += 1\n",
    "        \n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    print(\"Calculation time of Train_Cyclical: \" + str(end_time - start_time))\n",
    "    print(\"x-axis (steps) must be multiplied by 10 since the values recorded once in every 10 Cycle\")\n",
    "        \n",
    "    #print('J_epocs_train: {}'.format(J_epocs_train))\n",
    "    #print('Accuracy_epocs_train: {}'.format(Accuracy_epocs_train))\n",
    "    \n",
    "    Plot_Train_Validation_Cost_Accurracy(J_epocs_train, J_epocs_validation, Accuracy_epocs_train, Accuracy_epocs_validation)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_cost, eta, J_train, J_validation, A_train, A_validation\n",
      "\n",
      "epoch: 0, batch: 449, t: 450\n",
      "time: 2020-05-06 14:06:06.893180 ... operation: J_train\n"
     ]
    }
   ],
   "source": [
    "Train_Cyclical_Coarse_lambda1(network1, train_X_Norm, validation_X_Norm, 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(k_train == network1.train_y)/k_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "(W2star, b2star, G) =  linearLayer2.Backward(self, N, G, H, lambda_cost, W2, eta, layer_type='hidden')\n",
    "G = reluLayer.Backward(G)\n",
    "N = H.shape[1]\n",
    "(W1star, b1star) =  linearLayer1.Backward(self, N, G, X_batch, lambda_cost, W1, eta, layer_type='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TASK - 3 ###\n",
    "# Top-Level: After reading in and pre-processing the data, you can initialize the parameters of the model \n",
    "# W and b as you now know what size they should be. W has size Kxd and b is Kx1. Initialize each entry to have \n",
    "# Gaussian random values with zero mean and standard deviation .01. \n",
    "# You should use the Matlab function randn to create this data.\n",
    "asgn1.Task3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
